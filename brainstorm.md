1，当评估语言模型的推理效果时，不同版本模型的推理结果数量很可能并不是一致的。
比如对于相同一条推理请求，可能 modelA 进行了1次推理，而 modelB 进行了三次推理。

2，如何整合 openAI 等提供的一些开源评估方案？如 human-eval

3，对比不同来源的模型，比如公开模型，如 chatgpt、palm；如 triton 运行的模型；如 huggingface 运行的模型。

4，相同模型不同参数精度评估；不同模型相同请求推理结果对比；

5，模型微调之后的推理结果对比

6，可以使用 gpt-4 等参数等多的语言模型来评估较小参数量模型的推理质量

7，部分模型对 prompt 的格式有一定要求，那么在发送请求时，需要对 prompt 进行预处理，以满足模型的要求